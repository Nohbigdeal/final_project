{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3f09c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import selenium\n",
    "from urllib.parse import  urlparse\n",
    "import time\n",
    "import lxml\n",
    "from urllib.request import urlopen\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from contextlib import suppress\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_validate, KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f6909d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type      0\n",
      "review    7\n",
      "star      0\n",
      "label     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\not14\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\not14\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\not14\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "C:\\Users\\not14\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\not14\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\not14\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\not14\\anaconda3\\envs\\tf1\\lib\\site-packages\\pandas\\core\\series.py:4515: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  method=method,\n",
      "C:\\Users\\not14\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    }
   ],
   "source": [
    "ds_star = pd.read_csv(\"C:/netsong7/pythonwork/AI/finalproject/data cleaning(final)/shopping_ds.csv\")\n",
    "ds_star.drop_duplicates(subset=['type', 'review', 'star'], inplace=True)\n",
    "ds_star[\"label\"] = 0\n",
    "ds_star.loc[ds_star[\"star\"] > 3.5, \"label\"] = 1\n",
    "del ds_star[\"Unnamed: 0\"]\n",
    "ds_star_1_dummy, ds_star_1_train = train_test_split(ds_star[ds_star[\"label\"]==1], test_size=5432, random_state = 42)\n",
    "ds_star_0_train = ds_star[ds_star[\"label\"]==0]\n",
    "ds_star_balance = pd.concat([ds_star_1_train, ds_star_0_train])\n",
    "train_data, test_data = train_test_split(ds_star_balance, test_size=0.3, random_state = 0)\n",
    "train_data.drop_duplicates(subset=[\"review\", \"star\"], inplace=True)\n",
    "test_data.drop_duplicates(subset=[\"review\", \"star\"], inplace=True)\n",
    "train_data.loc[train_data.review.isnull()]\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "train_data['review'] = train_data['review'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "test_data.drop_duplicates(subset = ['review'], inplace=True) # 중복 제거\n",
    "test_data['review'] = test_data['review'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['review'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거\n",
    "train_data['review'] = train_data['review'].str.replace('^ +', \"\")\n",
    "train_data['review'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "835f75b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 276: expected 1 fields, saw 2\\n'\n",
      "C:\\Users\\not14\\anaconda3\\envs\\tf1\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "#stopwords = ['가','게','고','과','네','는','다','도','들','듯','를','에','와','으로',\n",
    "#             '은','을','의','이','인','임','자','잘','좀','지','하다','한']\n",
    "stopwords = pd.read_table(\"C:/netsong7/pythonwork/AI/stopwords1.txt\", encoding='utf-8-sig', header=None,error_bad_lines=False)\n",
    "stopwords = list(stopwords[0])\n",
    "\n",
    "X_train = []\n",
    "for sentence in train_data['review']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)\n",
    "\n",
    "X_test = []\n",
    "for sentence in test_data['review']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(temp_X)\n",
    "    \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "vocab_size = total_cnt - rare_cnt + 1    \n",
    "\n",
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])\n",
    "\n",
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "\n",
    "max_len = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e10861fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95c8d6c",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cab391ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "CLASS_COUNT = 10\n",
    "\n",
    "base_model = ResNet50(\n",
    "    weights='imagenet',\n",
    "    include_top=False, \n",
    "    input_shape=(224, 224, 3), \n",
    "    pooling='avg',\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "  base_model,\n",
    "  Dense(CLASS_COUNT, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a3c309ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6020 samples, validate on 1506 samples\n",
      "Epoch 1/15\n",
      "6020/6020 [==============================] - 64s 11ms/step - loss: 0.6031 - acc: 0.6934 - val_loss: 2.1129 - val_acc: 0.5212\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.52125, saving model to best_model.h5\n",
      "Epoch 2/15\n",
      "6020/6020 [==============================] - 61s 10ms/step - loss: 0.4721 - acc: 0.7831 - val_loss: 0.4948 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.52125 to 0.76560, saving model to best_model.h5\n",
      "Epoch 3/15\n",
      "6020/6020 [==============================] - 61s 10ms/step - loss: 0.4207 - acc: 0.8047 - val_loss: 0.4975 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76560\n",
      "Epoch 4/15\n",
      "6020/6020 [==============================] - 60s 10ms/step - loss: 0.3896 - acc: 0.8233 - val_loss: 0.4824 - val_acc: 0.7769\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76560 to 0.77689, saving model to best_model.h5\n",
      "Epoch 5/15\n",
      "6020/6020 [==============================] - 62s 10ms/step - loss: 0.3608 - acc: 0.8370 - val_loss: 0.4889 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.77689\n",
      "Epoch 6/15\n",
      "6020/6020 [==============================] - 62s 10ms/step - loss: 0.3351 - acc: 0.8490 - val_loss: 0.5081 - val_acc: 0.7583\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.77689\n",
      "Epoch 7/15\n",
      "6020/6020 [==============================] - 61s 10ms/step - loss: 0.3114 - acc: 0.8575 - val_loss: 0.6458 - val_acc: 0.7304\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.77689\n",
      "Epoch 8/15\n",
      "6020/6020 [==============================] - 61s 10ms/step - loss: 0.2897 - acc: 0.8693 - val_loss: 0.5583 - val_acc: 0.7537\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.77689\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_size, 100))\n",
    "model_lstm.add(LSTM(128))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model_lstm.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history_lstm = model_lstm.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8555b278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1171  468]\n",
      " [ 367 1196]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.71      0.74      1639\n",
      "           1       0.72      0.77      0.74      1563\n",
      "\n",
      "    accuracy                           0.74      3202\n",
      "   macro avg       0.74      0.74      0.74      3202\n",
      "weighted avg       0.74      0.74      0.74      3202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model_lstm.predict(X_test)\n",
    "y_pred_ = []\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i][0] > 0.5:\n",
    "        y_pred_.append(1)\n",
    "    else:\n",
    "        y_pred_.append(0)\n",
    "y_pred_\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6af6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d12e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "562f66b5",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bebca8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2c8a4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "CLASS_COUNT = 10\n",
    "\n",
    "base_model = ResNet50(\n",
    "    weights='imagenet',\n",
    "    include_top=False, \n",
    "    input_shape=(224, 224, 3), \n",
    "    pooling='avg',\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "  base_model,\n",
    "  Dense(CLASS_COUNT, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "926ac9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6020 samples, validate on 1506 samples\n",
      "Epoch 1/15\n",
      "6020/6020 [==============================] - 85s 14ms/step - loss: 0.5650 - acc: 0.7121 - val_loss: 0.4860 - val_acc: 0.7669\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76693, saving model to GRU_model.h5\n",
      "Epoch 2/15\n",
      "6020/6020 [==============================] - 80s 13ms/step - loss: 0.4496 - acc: 0.7826 - val_loss: 0.5576 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76693\n",
      "Epoch 3/15\n",
      "6020/6020 [==============================] - 78s 13ms/step - loss: 0.4140 - acc: 0.8022 - val_loss: 0.5293 - val_acc: 0.7457\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76693\n",
      "Epoch 4/15\n",
      "6020/6020 [==============================] - 77s 13ms/step - loss: 0.3815 - acc: 0.8231 - val_loss: 0.4903 - val_acc: 0.7576\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76693\n",
      "Epoch 5/15\n",
      "6020/6020 [==============================] - 78s 13ms/step - loss: 0.3575 - acc: 0.8332 - val_loss: 0.5167 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76693\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_GRU = Sequential()\n",
    "model_GRU.add(Embedding(vocab_size, 100))\n",
    "model_GRU.add(GRU(128))\n",
    "model_GRU.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('GRU_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model_GRU.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history_GRU = model_GRU.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "281e336c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1069  570]\n",
      " [ 283 1280]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.65      0.71      1639\n",
      "           1       0.69      0.82      0.75      1563\n",
      "\n",
      "    accuracy                           0.73      3202\n",
      "   macro avg       0.74      0.74      0.73      3202\n",
      "weighted avg       0.74      0.73      0.73      3202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model_GRU.predict(X_test)\n",
    "y_pred_ = []\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i][0] > 0.5:\n",
    "        y_pred_.append(1)\n",
    "    else:\n",
    "        y_pred_.append(0)\n",
    "y_pred_\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9839972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_GRU.save('model_GRU.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a1fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7674d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9260f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd282b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b4f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9acaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cd6fe07",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "86a560be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "43a9b158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7526, 100, 1)\n",
      "(3202, 100, 1)\n",
      "(7526,)\n",
      "(3202,)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "X_train_rnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "print(X_train_rnn.shape)\n",
    "\n",
    "X_test_rnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "print(X_test_rnn.shape)\n",
    "\n",
    "# y_data = np.concatenate((y_train, y_test))\n",
    "# y_data = to_categorical(y_data)\n",
    "# y_data\n",
    "\n",
    "# y_train = y_data[:10750]\n",
    "# y_test = y_data[10750:]\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f30ff7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_rnn():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=50, input_shape=(100, 1), return_sequences=False))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "    mc = ModelCheckpoint('GRU_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "28d66e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6020 samples, validate on 1506 samples\n",
      "Epoch 1/15\n",
      "6020/6020 [==============================] - 8s 1ms/step - loss: 0.6946 - acc: 0.5229 - val_loss: 0.6845 - val_acc: 0.5784\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.76693\n",
      "Epoch 2/15\n",
      "6020/6020 [==============================] - 3s 537us/step - loss: 0.6836 - acc: 0.5566 - val_loss: 0.6977 - val_acc: 0.5272\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76693\n",
      "Epoch 3/15\n",
      "6020/6020 [==============================] - 3s 516us/step - loss: 0.6825 - acc: 0.5650 - val_loss: 0.6804 - val_acc: 0.5684\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76693\n",
      "Epoch 4/15\n",
      "6020/6020 [==============================] - 3s 531us/step - loss: 0.6819 - acc: 0.5595 - val_loss: 0.6817 - val_acc: 0.5684\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76693\n",
      "Epoch 5/15\n",
      "6020/6020 [==============================] - 3s 529us/step - loss: 0.6818 - acc: 0.5601 - val_loss: 0.6819 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76693\n",
      "Epoch 6/15\n",
      "6020/6020 [==============================] - 3s 530us/step - loss: 0.6816 - acc: 0.5603 - val_loss: 0.6816 - val_acc: 0.5564\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76693\n",
      "Epoch 7/15\n",
      "6020/6020 [==============================] - 3s 534us/step - loss: 0.6814 - acc: 0.5586 - val_loss: 0.6887 - val_acc: 0.5179\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76693\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ca70f40188>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn = KerasClassifier(build_fn=vanilla_rnn, epochs=20, batch_size=50, verbose=1)\n",
    "model_rnn.fit(X_train_rnn, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "58811878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3202/3202 [==============================] - 5s 2ms/step\n",
      "[[ 308 1331]\n",
      " [ 203 1360]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.19      0.29      1639\n",
      "           1       0.51      0.87      0.64      1563\n",
      "\n",
      "    accuracy                           0.52      3202\n",
      "   macro avg       0.55      0.53      0.46      3202\n",
      "weighted avg       0.56      0.52      0.46      3202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model_rnn.predict(X_test_rnn)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a4ef62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6310b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c08ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acdc8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e14f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
